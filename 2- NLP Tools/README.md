# NLP Tools

Tokenization refers to a process by which a piece of sensitive data, such as a credit card number, is replaced by a surrogate value known as a token.

Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. 

Part-of-speech (POS) tagging is a process of converting a sentence to forms – list of words, list of tuples where each tuple is having a form (word, tag).


Stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.

Named entity recognition (NER) — sometimes referred to as entity chunking, extraction, or identification — is the task of identifying and categorizing key information (entities) in text.

Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.

The Matcher lets you find words and phrases using rules describing their token attributes. Rules can refer to token annotations

![image](https://user-images.githubusercontent.com/70816680/184263720-5c85e967-40b8-4d1c-bdcc-f83af62182c8.png)

