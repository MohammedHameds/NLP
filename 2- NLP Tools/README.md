# NLP Tools

Tokenization refers to a process by which a piece of sensitive data, such as a credit card number, is replaced by a surrogate value known as a token.

Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. 

![image](https://user-images.githubusercontent.com/70816680/184263720-5c85e967-40b8-4d1c-bdcc-f83af62182c8.png)

