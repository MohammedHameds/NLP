# NLP Tools

Tokenization refers to a process by which a piece of sensitive data, such as a credit card number, is replaced by a surrogate value known as a token.

Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. 

Part-of-speech (POS) tagging is a process of converting a sentence to forms â€“ list of words, list of tuples where each tuple is having a form (word, tag).


Stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.

![image](https://user-images.githubusercontent.com/70816680/184263720-5c85e967-40b8-4d1c-bdcc-f83af62182c8.png)

